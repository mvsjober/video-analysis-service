# Installation

```bash
sudo yum install redis
pip install flask celery[redis]
```

# Usage

Important: `--concurrency=1` flag to celery so that it runs only one job at a time.

```bash
sudo systemctl start redis
celery -A app.celery worker --concurrency=1
flask run  # in separate terminal
```

For development, `export FLASK_ENV=development` might also be handy.

Testing upload (example):

```bash
curl -F "file=@hello.txt" http://127.0.0.1:5000/upload
```

Start analysis (example):

```bash
curl -d "file_id=7b4758d4" http://127.0.0.1:5000/analysis
```

Check results (example):

```bash
curl http://127.0.0.1:5000/results/bbf6b9ce-4942-43e1-98b1-abc817bd9fe5
```


# API

## Upload file for analysis

The response will contain a `file_id` identifier (generated by server) which can
later be used to refer to the uploaded file.

POST `/file`

Payload: multipart/form-data format, file as `file` parameter

Response (example):

    HTTP 200
    { file_id: 123 }
    
## List uploaded files

GET `/file`

## Get info on one specific file

GET `/file/<file_id>`

The given file id can be the first characters of the SHA1, first match is returned.

## Delete file

Should we also have this? There needs to anyway be some kind of clean-up of old
files, e.g., after specific time, if running out of space, or similiar?

DELETE `/file/<file_id>` ?


## Start analysis for file

Start analysis for previously uploaded file. In theory we could start many
different types of analyses for the same file. Response will give `analysis_id`,
an identifier for later accessing the analysis results.

POST `/analyse`

Parameters (multipart/form-data format):

- `file_id` file id as received when uploading file
- `analysis` name of analysis

The rest of the parameters can be used as arguments for the analysis.

Response (HTTP 200):

    { analysis_id: 42 }

## Get analysis

Endpoint to poll if results are ready yet. We respond perhaps with HTTP 202 and
empty content if no result yet (but the analysis_id was a valid one). When result
is ready we return (HTTP 200) the results as JSON. The result can contain links to
files related to the analysis which need to be downloaded separately.

GET `/results/<analysis_id>`

Response (not ready yet):

    HTTP 202

Response (ready, HTTP 200):

    {
        result: [0.2, 0.99, -4.0],
        video_file: /download/analysis_42_pose.mp4
    }

## Get result files

GET `/download/analysis_42_pose.mp4`


# Implementation

- Database <del>or file system based only</del>? SQLite probably OK for this application

- How to handle analysis jobs? Is a Task Queue framework needed, such as [Celery](https://docs.celeryproject.org/en/stable/)?

## Upload file

- Store file on disk, e.g., based on `<file_id>`

- Check for duplicates with SHA-1 or similar?

- db table, `uploaded_file`:

  ```id | filename | status | sha-1```

- return `id` to user

## Analysis for file

- db table, `analysis`

  ```id | file_id | analysis_name | args | status```

- Implement 'dummy_N' analysis that just idles for N seconds.

    { file_id: 42, analysis: 'dummy', args: { 'seconds': 60 } }

- Return `id` to user

- How to report back completed analysis? Celery?

## Get analysis

Simply check status in database
